{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initial exploration\n",
    "\n",
    "You might want to keep track of your notebooks in the git repository. The easiest way to do this is to dump them into the notebooks folder. There are many different conventions that you can use, but here are a couple of useful ones:\n",
    "\n",
    "\n",
    "- `XX-{title}.ipynb` where `XX` is a number that indicates the order in which the notebooks were created. Useful for a smaller team where you dont want to clutter the directory with long file names.\n",
    "- `YYYMMDD-{initials}-{title}.ipynb` where `YYYMMDD` is the date on which the notebook was created, `initials` are your initials, and `title` is the title of the notebook. This is useful if you want to keep track of when the notebook was created and who created it.\n",
    "\n",
    "You might consider dumping notebooks into your user directory if they are meant to be a one-off and not meant to be reused as part of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell above, we add the magic for autoreloading modules. This is useful when you are calling in utility functions from your module. If you change the module, this should automatically reload the module on imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/16 21:09:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/16 21:09:41 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Galapagos:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>clef</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x748f22bc69c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_task_package.spark import get_spark\n",
    "\n",
    "spark = get_spark()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can call into into our module so we can organize a lot of the utilities into shared functions. Use this to your advantage to keep notebooks organized, and to limit the amount of actual code that you actually write in notebooks.\n",
    "\n",
    "One final tip that is useful is to use the `!` operator to run shell commands. This is useful when a shell utility is easier to use, or if you want to capture the output from your program to check into the repository. You can easily interpolate values from the python environment using `{}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs\t\t my_task_project.egg-info  README.md\t     tests\n",
      "LICENSE\t\t notebooks\t\t   requirements.txt  user\n",
      "my_task_package  pyproject.toml\t\t   scripts\n"
     ]
    }
   ],
   "source": [
    "project_root = \"../..\"\n",
    "! ls {project_root}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
