{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a178e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, pandas as pd, numpy as np, csv\n",
    "import requests\n",
    "import io\n",
    "import tarfile\n",
    "import zipfile\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91309c7d",
   "metadata": {},
   "source": [
    "### Load data for Hallucination Detection training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48dc40cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>grounding</th>\n",
       "      <th>generated_text</th>\n",
       "      <th>label</th>\n",
       "      <th>cut</th>\n",
       "      <th>dataset_origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91198</td>\n",
       "      <td>Colin Kaepernick . Kaepernick began his profes...</td>\n",
       "      <td>Colin Kaepernick became a starting quarterback...</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>Fever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194462</td>\n",
       "      <td>Katherine Matilda `` Tilda '' Swinton ( born 5...</td>\n",
       "      <td>Tilda Swinton is a vegan.</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>Fever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137334</td>\n",
       "      <td>Soul Food is a 1997 American comedy-drama film...</td>\n",
       "      <td>Fox 2000 Pictures released the film Soul Food.</td>\n",
       "      <td>1</td>\n",
       "      <td>val</td>\n",
       "      <td>Fever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>166626</td>\n",
       "      <td>Anne Rice . Born in New Orleans , Rice spent m...</td>\n",
       "      <td>Anne Rice was born in New Jersey.</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Fever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111897</td>\n",
       "      <td>Telemundo ( [ teleˈmundo ] ) is an American Sp...</td>\n",
       "      <td>Telemundo is a English-language television net...</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>Fever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                          grounding  \\\n",
       "0   91198  Colin Kaepernick . Kaepernick began his profes...   \n",
       "1  194462  Katherine Matilda `` Tilda '' Swinton ( born 5...   \n",
       "2  137334  Soul Food is a 1997 American comedy-drama film...   \n",
       "3  166626  Anne Rice . Born in New Orleans , Rice spent m...   \n",
       "4  111897  Telemundo ( [ teleˈmundo ] ) is an American Sp...   \n",
       "\n",
       "                                      generated_text  label   cut  \\\n",
       "0  Colin Kaepernick became a starting quarterback...      0   val   \n",
       "1                          Tilda Swinton is a vegan.      0   val   \n",
       "2     Fox 2000 Pictures released the film Soul Food.      1   val   \n",
       "3                  Anne Rice was born in New Jersey.      0  test   \n",
       "4  Telemundo is a English-language television net...      0   val   \n",
       "\n",
       "  dataset_origin  \n",
       "0          Fever  \n",
       "1          Fever  \n",
       "2          Fever  \n",
       "3          Fever  \n",
       "4          Fever  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory where the CSV files are stored\n",
    "data_dir = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "\n",
    "# List all CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Load all CSV files into a single DataFrame\n",
    "df_list = []\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "training_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07cc00c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut\n",
      "val     84044\n",
      "test    38332\n",
      "Name: count, dtype: int64\n",
      "val can be used for training the model and test can be used for evaluation the performance\n"
     ]
    }
   ],
   "source": [
    "# Get the counts of val and test data\n",
    "val_test_spit = training_df['cut'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(val_test_spit)\n",
    "\n",
    "print(\"val can be used for training the model and test can be used for evaluation the performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d27a5bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset_origin\n",
       "Vitamin C     63054\n",
       "HaluEval      20000\n",
       "Fever         19998\n",
       "PAWS           8000\n",
       "XSumFaith      2353\n",
       "SummEval       1698\n",
       "FactCC         1434\n",
       "FRANK          1393\n",
       "Polytope       1268\n",
       "Cao22           696\n",
       "CLIFF           600\n",
       "TofuEval        534\n",
       "Wang20          474\n",
       "samsum          250\n",
       "qags_xsum       239\n",
       "qags_cnndm      235\n",
       "Goyal21         150\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the counts by dataset origin\n",
    "training_df['dataset_origin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f68b9ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train and test - remove vitamin c & Fever as it is skewing the dataset towards Fact verification\n",
    "# train lists \n",
    "train_data = training_df[(training_df.cut == 'val') & (~training_df['dataset_origin'].isin(['Vitamin C', 'Fever']))]\n",
    "train_grounding_list = list(train_data['grounding'])\n",
    "train_generated_list = list(train_data['generated_text'])\n",
    "\n",
    "# test lists\n",
    "test_data = training_df[(training_df.cut == 'test') & (~training_df['dataset_origin'].isin(['Vitamin C', 'Fever']))]\n",
    "test_grounding_list = list(test_data['grounding'])\n",
    "test_generated_list = list(test_data['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a465d",
   "metadata": {},
   "source": [
    "### LLM as judge baseline Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fa47120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Tuple, Dict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import json\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "class BatchLLMjudge:\n",
    "    def __init__(self,\n",
    "                 #nli_model_name: str = \"roberta-large-mnli\",\n",
    "                 batch_size: int = 32,\n",
    "                 kg_construction_prompt=\"\"\"You are an expert at determining if a summary is consistent with a source article. Given an article and a summary, determine if all the information in the summary is supported by the article. Answer \"yes\" if the summary is consistent, and \"no\" if it is inconsistent.\"\"\",\n",
    "                 ei_format_prompt=\"\"\"Article: {article}\n",
    "    Summary: {summary}\n",
    "    Answer (yes or no):\"\"\", #Changed name of prompt\n",
    "                 kg_tips_prompt=\"\",  #Not needed for this task\n",
    "                 kg_examples_prompt=\"\", #Not needed for this task\n",
    "                 llm_model: str = \"llama-3.3-70b-versatile\"):  #Using groq model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
    "        #self.nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name).to(self.device)\n",
    "        self.batch_size = batch_size\n",
    "        self.kg_construction_prompt = kg_construction_prompt\n",
    "        self.ei_format_prompt = ei_format_prompt #Changed\n",
    "        self.kg_tips_prompt = kg_tips_prompt\n",
    "        self.kg_examples_prompt = kg_examples_prompt\n",
    "        self.llm_model_name = llm_model  # Store LLM model name\n",
    "        self.groq_client = Groq(api_key='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',)  #Groq client\n",
    "        # No pipeline needed for groq api\n",
    "\n",
    "\n",
    "    def construct_ei_batch(self, articles: List[str], summaries: List[str]) -> List[str]: #ei for entailment inference #Changed name and parameters\n",
    "        # Builds prompts for entailment inference\n",
    "        prompts = [self.kg_construction_prompt + \"\\n\" + self.ei_format_prompt.format(article=article, summary=summary) for article, summary in zip(articles, summaries)]  #Changed to ei\n",
    "        return prompts #Returning list of prompts\n",
    "\n",
    "\n",
    "    def call_llm_to_extract_kg(self,prompt: str) -> str: #Changed name and return type\n",
    "      # Wrap the LLM call in a try-except block\n",
    "        try:\n",
    "            #Call Groq API\n",
    "            chat_completion = self.groq_client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=self.llm_model_name,\n",
    "                max_tokens = 10, #Reduced tokens, just need a yes/no\n",
    "            )\n",
    "            output = chat_completion.choices[0].message.content\n",
    "            return output  #Return the direct output, no parsing needed\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling LLM: {e}\")\n",
    "            return \"\"  #Return empty string in case of error\n",
    "\n",
    "    def check_consistency_batch(self, grok_outputs: List[str]) -> List[int]:  #Now the grok outputs is what comes in\n",
    "\n",
    "        results = []\n",
    "        for output in grok_outputs:\n",
    "          output = output.lower()\n",
    "          if \"yes\" in output:\n",
    "            results.append(1)  #Consistent\n",
    "          elif \"no\" in output:\n",
    "            results.append(0)  #Inconsistent\n",
    "          else:\n",
    "            results.append(-1)  #Undetermined, handle as needed.\n",
    "\n",
    "        return results\n",
    "\n",
    "    def evaluate_batch(self, articles: List[str], summaries: List[str]) -> List[int]: #Added labels\n",
    "        prompts = self.construct_ei_batch(articles, summaries)\n",
    "        #Process by batches\n",
    "        all_grok_outputs = []\n",
    "        for i in range(0, len(prompts), self.batch_size):\n",
    "            batch_prompts = prompts[i:i + self.batch_size]\n",
    "            batch_grok_outputs = [self.call_llm_to_extract_kg(prompt) for prompt in batch_prompts] #Call grok for each prompt\n",
    "            all_grok_outputs.extend(batch_grok_outputs)\n",
    "\n",
    "\n",
    "        #Evaluate the outputs\n",
    "        predicted_labels = self.check_consistency_batch(all_grok_outputs) #Check the concistency\n",
    "        \n",
    "        #Calculate metrics (Accuracy, etc).\n",
    "        #correct_predictions = sum([1 for i in range(len(labels)) if labels[i] == predicted_labels[i]]) #Compare predictions with labels\n",
    "\n",
    "        #accuracy = correct_predictions / len(labels) if len(labels) > 0 else 0 #Calculate accurancy\n",
    "        #Create the results\n",
    "        #results = {\n",
    "        #    \"accuracy\": accuracy,\n",
    "        #    \"predicted_labels\": predicted_labels,\n",
    "        #    \"actual_labels\": labels\n",
    "        #}\n",
    "        return predicted_labels\n",
    "\n",
    "# Usage example\n",
    "# Ensure GROQ_API_KEY is set in your environment variables\n",
    "# export GROQ_API_KEY=\"YOUR_GROQ_API_KEY\"\n",
    "batch_llm_eval = BatchLLMjudge(llm_model=\"llama-3.3-70b-versatile\") #Specify Groq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e9fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f0739",
   "metadata": {},
   "source": [
    "###### running LLM as judge on sampled train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8954aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample train and test data for llm as judge approach due to bottleneck on API call limit\n",
    "\n",
    "# Randomly sample 20% of the rows\n",
    "sampled_train_data = train_data.sample(frac=0.2, random_state=1)\n",
    "sampled_test_data = test_data.sample(frac=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23f66fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check prompt generation\n",
    "train_grounding_list = list(sampled_train_data['grounding'])\n",
    "train_generated_list = list(sampled_train_data['generated_text'])\n",
    "prompt_data = batch_llm_eval.construct_ei_batch(train_grounding_list, train_generated_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af11e6fb",
   "metadata": {},
   "source": [
    "#### run aynchronous batch processing on Groq Api call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e5649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for batch processing - https://console.groq.com/docs/batch\n",
    "# 1. create json file\n",
    "# 2. upload json file\n",
    "# 3. create batch job\n",
    "# 4. check batch status\n",
    "# 5. reterive batch results\n",
    "\n",
    "## Consider splitting very large workloads into multiple smaller batches (e.g. 1000 requests per batch) \n",
    "## for a better chance at completion rather than expiration for when we are under heavy load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e48be31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "16\n",
      "32\n",
      "48\n",
      "64\n",
      "80\n",
      "96\n",
      "112\n",
      "128\n",
      "144\n",
      "160\n",
      "176\n",
      "192\n",
      "208\n",
      "224\n",
      "240\n",
      "256\n",
      "272\n",
      "288\n",
      "304\n",
      "320\n",
      "336\n",
      "352\n",
      "368\n",
      "384\n",
      "400\n",
      "416\n",
      "432\n",
      "448\n",
      "464\n",
      "480\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000024, Requested 2394. Please try again in 3m28.980999999s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000023, Requested 1039. Please try again in 1m31.808s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000010, Requested 1951. Please try again in 2m49.5122s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000009, Requested 797. Please try again in 1m9.7086s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000018, Requested 748. Please try again in 1m6.2582s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000017, Requested 1374. Please try again in 2m0.2396s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000016, Requested 875. Please try again in 1m17.033999999s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999987, Requested 2554. Please try again in 3m39.5128s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "496\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000012, Requested 1339. Please try again in 1m56.8012s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "512\n",
      "528\n",
      "544\n",
      "560\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999473, Requested 1627. Please try again in 1m34.986399999s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999472, Requested 2411. Please try again in 2m42.621s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "576\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999751, Requested 1092. Please try again in 1m12.7732s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999750, Requested 1242. Please try again in 1m25.6712s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "592\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000011, Requested 1994. Please try again in 2m53.2564s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999986, Requested 2108. Please try again in 3m0.8536s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999985, Requested 1749. Please try again in 2m29.77s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999984, Requested 880. Please try again in 1m14.623399999s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999929, Requested 1067. Please try again in 1m25.9874s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000005, Requested 937. Please try again in 1m21.4606s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000005, Requested 1363. Please try again in 1m58.205s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000004, Requested 1183. Please try again in 1m42.589s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000003, Requested 1430. Please try again in 2m3.8668s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000002, Requested 954. Please try again in 1m22.6784s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000002, Requested 934. Please try again in 1m20.885399999s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000001, Requested 1387. Please try again in 1m59.958599999s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999756, Requested 2598. Please try again in 3m23.369999999s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999923, Requested 1511. Please try again in 2m3.8326s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000003, Requested 1210. Please try again in 1m44.8246s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000002, Requested 807. Please try again in 1m9.927399999s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000001, Requested 2404. Please try again in 3m27.8452s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "624\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999984, Requested 722. Please try again in 1m0.932399999s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999817, Requested 1598. Please try again in 2m2.2404s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999816, Requested 1242. Please try again in 1m31.369s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999815, Requested 2347. Please try again in 3m6.737s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999814, Requested 925. Please try again in 1m3.7732s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999971, Requested 970. Please try again in 1m21.2858s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000011, Requested 1623. Please try again in 2m21.2454s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 1000010, Requested 962. Please try again in 1m24.011s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "640\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999862, Requested 917. Please try again in 1m7.291s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error calling LLM: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-1b-preview` in organization `org_01jpw0vz73e8arvgjkbgyppv3y` service tier `on_demand` on tokens per day (TPD): Limit 1000000, Used 999861, Requested 1535. Please try again in 2m0.5942s. Need more tokens? Visit https://groq.com/self-serve-support/ to request higher limits.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/dsgt_cleff_simpltxt_2/lib/python3.13/site-packages/groq/_base_client.py:999\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 999\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dsgt_cleff_simpltxt_2/lib/python3.13/site-packages/httpx/_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     train_grounding_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrounding\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m     train_generated_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m     result \u001b[38;5;241m=\u001b[39m batch_graph_eval\u001b[38;5;241m.\u001b[39mevaluate_batch(train_grounding_list, train_generated_list)\n\u001b[1;32m     15\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mextend(result)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Store the results as a new column in the original DataFrame\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 74\u001b[0m, in \u001b[0;36mBatchGraphEval.evaluate_batch\u001b[0;34m(self, articles, summaries)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(prompts), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[1;32m     73\u001b[0m     batch_prompts \u001b[38;5;241m=\u001b[39m prompts[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[0;32m---> 74\u001b[0m     batch_grok_outputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_llm_to_extract_kg(prompt) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m batch_prompts] \u001b[38;5;66;03m#Call grok for each prompt\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     all_grok_outputs\u001b[38;5;241m.\u001b[39mextend(batch_grok_outputs)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m#Evaluate the outputs\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 42\u001b[0m, in \u001b[0;36mBatchGraphEval.call_llm_to_extract_kg\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_llm_to_extract_kg\u001b[39m(\u001b[38;5;28mself\u001b[39m,prompt: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m: \u001b[38;5;66;03m#Changed name and return type\u001b[39;00m\n\u001b[1;32m     39\u001b[0m   \u001b[38;5;66;03m# Wrap the LLM call in a try-except block\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;66;03m#Call Groq API\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m         chat_completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroq_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     43\u001b[0m             messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}],\n\u001b[1;32m     44\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_model_name,\n\u001b[1;32m     45\u001b[0m             max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, \u001b[38;5;66;03m#Reduced tokens, just need a yes/no\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         )\n\u001b[1;32m     47\u001b[0m         output \u001b[38;5;241m=\u001b[39m chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output  \u001b[38;5;66;03m#Return the direct output, no parsing needed\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dsgt_cleff_simpltxt_2/lib/python3.13/site-packages/groq/resources/chat/completions.py:322\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, seed, service_tier, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    199\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    324\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    325\u001b[0m             {\n\u001b[1;32m    326\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    327\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    328\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    329\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    330\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    333\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m    334\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    335\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    336\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    337\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    338\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_format,\n\u001b[1;32m    339\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    340\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    341\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    342\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    343\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    344\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    345\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    346\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    347\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    348\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    349\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    350\u001b[0m             },\n\u001b[1;32m    351\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    352\u001b[0m         ),\n\u001b[1;32m    353\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    354\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    355\u001b[0m         ),\n\u001b[1;32m    356\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    357\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    359\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/dsgt_cleff_simpltxt_2/lib/python3.13/site-packages/groq/_base_client.py:1225\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1213\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1222\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1223\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1224\u001b[0m     )\n\u001b[0;32m-> 1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/.conda/envs/dsgt_cleff_simpltxt_2/lib/python3.13/site-packages/groq/_base_client.py:917\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 917\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    918\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    919\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    920\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    921\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    922\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m    923\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/dsgt_cleff_simpltxt_2/lib/python3.13/site-packages/groq/_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1006\u001b[0m         input_options,\n\u001b[1;32m   1007\u001b[0m         cast_to,\n\u001b[1;32m   1008\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1009\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1010\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1011\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/.conda/envs/dsgt_cleff_simpltxt_2/lib/python3.13/site-packages/groq/_base_client.py:1052\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1048\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m-> 1052\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1055\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1056\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1059\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1060\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# sampled training data run\n",
    "# Batch size\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "# Loop through DataFrame in batches\n",
    "predictions = []\n",
    "for start in range(0, len(sampled_train_data), batch_size):\n",
    "    print(start)\n",
    "    end = start + batch_size\n",
    "    batch = sampled_train_data.iloc[start:end]\n",
    "    train_grounding_list = list(batch['grounding'])\n",
    "    train_generated_list = list(batch['generated_text'])\n",
    "    result = batch_graph_eval.evaluate_batch(train_grounding_list, train_generated_list)\n",
    "    predictions.extend(result)\n",
    "\n",
    "# Store the results as a new column in the original DataFrame\n",
    "sampled_train_data['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57bf08cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "731511c1",
   "metadata": {},
   "source": [
    "###### Training data results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a707c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(sampled_train_data['label'])\n",
    "predicted_labels = list(sampled_train_data['prediction'])\n",
    "correct_predictions = sum([1 for i in range(len(labels)) if labels[i] == predicted_labels[i]]) \n",
    "\n",
    "tp = sum([1 for i in range(len(labels)) if labels[i] == predicted_labels[i] and predicted_labels[i] == 1]) \n",
    "fp = sum([1 for i in range(len(labels)) if labels[i] != predicted_labels[i] and predicted_labels[i] == 1]) \n",
    "tn = sum([1 for i in range(len(labels)) if labels[i] == predicted_labels[i] and predicted_labels[i] == 0]) \n",
    "fn = sum([1 for i in range(len(labels)) if labels[i] != predicted_labels[i] and predicted_labels[i] == 0]) \n",
    "\n",
    "print(\"TP :\", tp)\n",
    "print(\"FP :\", fp)\n",
    "print(\"TN :\", tn)\n",
    "print(\"FN :\", fn)\n",
    "\n",
    "print(\"Accuracy :\" (tp+tn)/len(labels))\n",
    "print(\"Precision :\" (tp)/(tp+fp))\n",
    "print(\"Recall :\" (tp)/(tp+fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ff6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "# Loop through DataFrame in batches\n",
    "predictions = []\n",
    "for start in range(0, len(sampled_test_data), batch_size):\n",
    "    print(start)\n",
    "    end = start + batch_size\n",
    "    batch = sampled_test_data.iloc[start:end]\n",
    "    train_grounding_list = list(batch['grounding'])\n",
    "    train_generated_list = list(batch['generated_text'])\n",
    "    result = batch_graph_eval.evaluate_batch(train_grounding_list, train_generated_list)\n",
    "    predictions.extend(result)\n",
    "\n",
    "# Store the results as a new column in the original DataFrame\n",
    "sampled_test_data['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff70bf02",
   "metadata": {},
   "source": [
    "###### Test data results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6bc1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(sampled_test_data['label'])\n",
    "predicted_labels = list(sampled_test_data['prediction'])\n",
    "correct_predictions = sum([1 for i in range(len(labels)) if labels[i] == predicted_labels[i]]) \n",
    "\n",
    "tp = sum([1 for i in range(len(labels)) if labels[i] == predicted_labels[i] and predicted_labels[i] == 1]) \n",
    "fp = sum([1 for i in range(len(labels)) if labels[i] != predicted_labels[i] and predicted_labels[i] == 1]) \n",
    "tn = sum([1 for i in range(len(labels)) if labels[i] == predicted_labels[i] and predicted_labels[i] == 0]) \n",
    "fn = sum([1 for i in range(len(labels)) if labels[i] != predicted_labels[i] and predicted_labels[i] == 0]) \n",
    "\n",
    "print(\"TP :\", tp)\n",
    "print(\"FP :\", fp)\n",
    "print(\"TN :\", tn)\n",
    "print(\"FN :\", fn)\n",
    "\n",
    "print(\"Accuracy :\" (tp+tn)/len(labels))\n",
    "print(\"Precision :\" (tp)/(tp+fp))\n",
    "print(\"Recall :\" (tp)/(tp+fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e334df",
   "metadata": {},
   "source": [
    "### 1. GraphEval Implementation as Baseline Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c5871",
   "metadata": {},
   "source": [
    "GraphEval is a combination approach of using LLMs to create KGs and check consistency using NLI to detect hallucinations.\n",
    "\n",
    "The implementation includes the main components of GraphEval as described in the paper:\n",
    "1.KG construction from the LLM output\n",
    "2.Consistency checking for each triple using an NLI model\n",
    "3.Overall evaluation based on the consistency of all triples\n",
    "\n",
    "Note that the KG construction step (construct_kg method) is a placeholder and should be implemented using an actual LLM in practice. The paper doesn't provide specific details on this step, so you would need to design an appropriate prompt and use an LLM API to generate the KG triples.\n",
    "\n",
    "The check_consistency method uses a pre-trained RoBERTa model fine-tuned on MNLI for natural language inference. It returns the probability of contradiction between the triple and the context.\n",
    "\n",
    "The evaluate method puts it all together, constructing the KG, checking each triple for consistency, and returning the overall result along with any inconsistent triples found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b8af3c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are crucial in today's technology landscape due to their numerous benefits and applications. Here are some reasons why fast language models are important:\n",
      "\n",
      "1. **Improved User Experience**: Fast language models enable quick and accurate processing of natural language inputs, allowing users to interact with systems more efficiently. This leads to a better overall user experience, as users can receive responses and results rapidly.\n",
      "2. **Real-time Applications**: Fast language models are essential for real-time applications, such as:\n",
      "\t* Chatbots and virtual assistants, which require rapid responses to user queries.\n",
      "\t* Sentiment analysis and opinion mining, where speed is critical for timely decision-making.\n",
      "\t* Language translation, where fast processing enables real-time communication across languages.\n",
      "3. **Efficient Processing of Large Datasets**: Fast language models can handle massive amounts of text data, making them ideal for applications such as:\n",
      "\t* Text classification and categorization.\n",
      "\t* Named entity recognition and extraction.\n",
      "\t* Topic modeling and document clustering.\n",
      "4. **Competitive Advantage**: In industries like customer service, fast language models can provide a competitive advantage by enabling companies to respond quickly to customer inquiries, resolving issues faster, and improving overall customer satisfaction.\n",
      "5. **Enhanced Decision-Making**: Fast language models can help organizations make data-driven decisions by quickly analyzing and processing large amounts of text data, such as:\n",
      "\t* Social media posts and reviews.\n",
      "\t* Customer feedback and complaints.\n",
      "\t* Market research and trends analysis.\n",
      "6. **Reducing Latency**: Fast language models can significantly reduce latency in applications, enabling faster response times and improving overall system performance. This is particularly important in applications where latency can have significant consequences, such as in healthcare or finance.\n",
      "7. **Improved Accessibility**: Fast language models can help bridge the language gap by enabling real-time language translation and facilitating communication across languages and cultures.\n",
      "8. **Supporting Edge AI**: Fast language models are essential for edge AI applications, where AI models are deployed on devices or at the edge of the network, requiring fast and efficient processing to minimize latency and bandwidth usage.\n",
      "9. **Enabling Multi-Modal Interactions**: Fast language models can enable multi-modal interactions, such as voice, text, and gesture-based interactions, which require fast processing to provide seamless and intuitive user experiences.\n",
      "10. **Driving Innovation**: Fast language models can drive innovation in areas such as:\n",
      "\t* Conversational AI.\n",
      "\t* Human-computer interaction.\n",
      "\t* Natural language processing.\n",
      "\n",
      "In summary, fast language models are essential for various applications, from improving user experience to driving innovation. Their ability to process natural language inputs quickly and accurately enables a wide range of use cases, making them a critical component of modern technology infrastructure.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key='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',\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Tuple, Dict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import json\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "class BatchGraphEval:\n",
    "    def __init__(self,\n",
    "                 nli_model_name: str = \"roberta-large-mnli\",\n",
    "                 batch_size: int = 32,\n",
    "                 kg_construction_prompt=\"\"\"You are an expert at extracting information in structured formats to build a knowledge graph. \n",
    "    Step 1 − Entity detection: Identify all entities in the raw text. Make sure not to miss any out. Entities should be basic and simple, they are akin to Wikipedia nodes. \n",
    "    Step 2 − Coreference resolution: Find all expressions in the text that refer to the same entity. Make sure entities are not duplicated. \n",
    "    In particular do not include entities that are more specific versions themselves, e.g. \"a detailed view of jupiter’s atmosphere\" and \"jupiter’s atmosphere\", only include the most specific version of the entity. \n",
    "    Step 3 − Relation extraction: Identify semantic relationships between the entities you have identified.\n",
    "    Format: Return the knowledge graph as a list of triples, i.e. [ \"entity1\", \"relation1−2\", \"entity2\"], in Python code.\"\"\",\n",
    "                 kg_format_prompt=\"\"\"Use the given format to extract information from the following input: <input>{input}</input>.\n",
    "    Skip the preamble and output the result as a list within <python> tags.\"\"\",\n",
    "                 kg_tips_prompt=\"\"\"Important Tips:\n",
    "    1. Make sure all information is included in the knowledge graph.\n",
    "    2. Each triple must only contain three strings! None of the strings should be empty.\n",
    "    3. Do not split up related information into separate triples because this could change the meaning.\n",
    "    4. Make sure all brackets and quotation marks are matched.\n",
    "    5. Before adding a triplet to the knowledge graph, check the concatenated triple makes sense as a sentence. If not, discard it.\"\"\",\n",
    "                 kg_examples_prompt=\"\"\"Here are some example input and output pairs.\n",
    "    ## Example 1.\n",
    "    Input: \"The Walt Disney Company, commonly known as Disney, is an American multinational mass media and entertainment conglomerate that is headquartered at the Walt Disney Studios complex in Burbank, California.\"\n",
    "    Output: [ [ \"The Walt Disney Company\", \"headquartered at\", \"Walt Disney Studios complex in Burbank, California\" ], [ \"The Walt Disney Company\", \"commonly known as\", \"Disney\" ], [ \"The Walt Disney Company\", \"instance of\", \"American multinational mass media and entertainment conglomerate\" ] ]\n",
    "    ## Example 2.\n",
    "    Input: \"Amanda Jackson was born in Springfield, Ohio, USA on June 1, 1985. She was a basketball player for the U.S. women’s team.\"\n",
    "    Output: [ [ \"Amanda Jackson\", \"born in\", \"Springfield, Ohio, USA\" ], [ \"Amanda Jackson\", \"born on\", \"June 1, 1985\" ], [ \"Amanda Jackson\", \"occupation\", \"basketball player\" ], [ \"Amanda Jackson\", \"played for\", \"U.S. women’s basketball team\" ] ]\n",
    "    ## Example 3.\n",
    "    Input: \"Music executive Darius Van Arman was born in Pennsylvania. He attended Gonzaga College High School and is a human being.\"\n",
    "    Output: [ [ \"Darius Van Arman\", \"occupation\", \"Music executive\" ], [ \"Darius Van Arman\", \"born in\", \"Pennsylvania\" ], [ \"Darius Van Arman\", \"attended\", \"Gonzaga College High School\" ], [ \"Darius Van Arman\", \"instance of\", \"human being\" ] ]\n",
    "    ## Example 4.\n",
    "    Input: \"Italy had 3.6x times more cases of coronavirus than China.\"\n",
    "    Output: [ [ \"Italy\", \"had 3.6x times more cases of coronavirus than\", \"China\" ] ]\n",
    "    \"\"\",\n",
    "                 llm_model: str = \"llama-3.3-70b-versatile\"):  #Using groq model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
    "        self.nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name).to(self.device)\n",
    "        self.batch_size = batch_size\n",
    "        self.kg_construction_prompt = kg_construction_prompt\n",
    "        self.kg_format_prompt = kg_format_prompt\n",
    "        self.kg_tips_prompt = kg_tips_prompt\n",
    "        self.kg_examples_prompt = kg_examples_prompt\n",
    "        self.llm_model_name = llm_model  # Store LLM model name\n",
    "        self.groq_client = Groq(api_key='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',)  #Groq client\n",
    "        # No pipeline needed for groq api\n",
    "\n",
    "\n",
    "    def construct_kg_batch(self, llm_outputs: List[str]) -> List[List[Tuple[str, str, str]]]:\n",
    "        # Use the prompt with the LLM to construct KGs for multiple outputs\n",
    "        batch_kgs = []\n",
    "        for output in llm_outputs:\n",
    "          input_text = f\"{self.kg_construction_prompt} {self.kg_format_prompt.format(input=output)} {self.kg_tips_prompt} {self.kg_examples_prompt}\"\n",
    "          #print(input_text)\n",
    "            #In practice, you would call an LLM API here with the combined prompt\n",
    "            #and process the output to extract the KG triples.\n",
    "            #Replace this with the actual LLM call\n",
    "          triples = self.call_llm_to_extract_kg(input_text)\n",
    "          batch_kgs.append(triples)\n",
    "\n",
    "        return batch_kgs\n",
    "\n",
    "    def call_llm_to_extract_kg(self,prompt: str) -> List[Tuple[str, str, str]]:\n",
    "      # Wrap the LLM call in a try-except block\n",
    "        try:\n",
    "            #Call Groq API\n",
    "            chat_completion = self.groq_client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=self.llm_model_name,\n",
    "            )\n",
    "            output = chat_completion.choices[0].message.content\n",
    "            \n",
    "\n",
    "            # Extract the knowledge graph from the output\n",
    "            # Assumes the LLM returns the KG in a list within <python> tags\n",
    "            start_tag = output.find('[')\n",
    "            end_tag = output.rfind(']')\n",
    "            if start_tag != -1 and end_tag != -1:\n",
    "                kg_string = output[start_tag:end_tag+1]\n",
    "                try:\n",
    "                    kg = eval(kg_string) #use literal_eval for security\n",
    "                    if isinstance(kg, list):\n",
    "                        return kg\n",
    "                    else:\n",
    "                        print(\"LLM did not return a list.\")\n",
    "                        return []\n",
    "                except (SyntaxError, NameError) as e:\n",
    "                    print(f\"Error parsing LLM output: {e}\")\n",
    "                    return []\n",
    "            else:\n",
    "                print(\"Could not find KG in LLM output.\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling LLM: {e}\")\n",
    "            return []\n",
    "\n",
    "    def check_consistency_batch(self, triples: List[Tuple[str, str, str]], contexts: List[str]) -> List[float]:\n",
    "        # Combine the triples into sentences\n",
    "        triple_texts = [f\"{t[0]} {t[1]} {t[2]}\" for t in triples]\n",
    "\n",
    "        # Tokenize the inputs\n",
    "        inputs = self.tokenizer(triple_texts, contexts, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        # Get the model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.nli_model(**inputs)\n",
    "        probs = outputs.logits.softmax(dim=-1)\n",
    "\n",
    "        # Return the probabilities of contradiction (index 2 in RoBERTa MNLI model)\n",
    "        return probs[:, 2].tolist()\n",
    "\n",
    "    def evaluate_batch(self, batch_kgs: List[List[Tuple[str, str, str]]], contexts: List[str]) -> List[int]:\n",
    "        #batch_kgs = self.construct_kg_batch(llm_outputs)\n",
    "        results = []\n",
    "\n",
    "        for i in range(0, len(llm_outputs), self.batch_size):\n",
    "            batch_llm_outputs = llm_outputs[i:i+self.batch_size]\n",
    "            batch_contexts = contexts[i:i+self.batch_size]\n",
    "            batch_kgs_subset = batch_kgs[i:i+self.batch_size]\n",
    "\n",
    "            batch_triples = [triple for idx,kg in enumerate(batch_kgs_subset) for triple in kg]\n",
    "\n",
    "            batch_contexts_expanded = []\n",
    "\n",
    "            #Iterate over batch of kgs\n",
    "            for batch_idx, kg in enumerate(batch_kgs_subset):\n",
    "\n",
    "              #Extend context for each set of triples within a kg\n",
    "              batch_contexts_expanded.extend([batch_contexts[batch_idx]] * len(kg))\n",
    "\n",
    "            inconsistency_probs = self.check_consistency_batch(batch_triples, batch_contexts_expanded)\n",
    "\n",
    "            triple_index = 0\n",
    "\n",
    "            for batch_idx, kg in enumerate(batch_kgs_subset):\n",
    "\n",
    "                inconsistent_triples = []\n",
    "\n",
    "                for triple in kg:\n",
    "                    inconsistency_prob = inconsistency_probs[triple_index]\n",
    "                    if inconsistency_prob > 0.5:\n",
    "                        inconsistent_triples.append((triple, inconsistency_prob))\n",
    "                    triple_index += 1\n",
    "\n",
    "                if len(inconsistent_triples) > 0:\n",
    "                    result.append(0)\n",
    "                else:\n",
    "                    result.append(1)\n",
    "                    1 else 0 end\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bcb70bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 5)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 8)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unterminated string literal (detected at line 8) (<string>, line 8)\n",
      "Error parsing LLM output: unterminated string literal (detected at line 10) (<string>, line 10)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 7)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: name 'entity1' is not defined\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 8)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 2)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 6)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 5)\n",
      "Error parsing LLM output: unterminated string literal (detected at line 15) (<string>, line 15)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 2)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 5)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 6)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 9)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 6)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unterminated string literal (detected at line 6) (<string>, line 6)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Could not find KG in LLM output.\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: name 'entity1' is not defined\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 5)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 6)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 9)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 8)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 5)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 2)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 2)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unterminated string literal (detected at line 5) (<string>, line 5)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 6)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: unterminated string literal (detected at line 8) (<string>, line 8)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 2)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 2)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n"
     ]
    }
   ],
   "source": [
    "batch_graph_eval = BatchGraphEval(llm_model=\"llama-3.3-70b-versatile\")\n",
    "train_llm_kgs = batch_graph_eval.construct_kg_batch(train_generated_list[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9155beaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 5)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 8)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unmatched ')' (<string>, line 3)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 9)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 6)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unterminated string literal (detected at line 6) (<string>, line 6)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unterminated string literal (detected at line 6) (<string>, line 6)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 8)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 2)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unterminated string literal (detected at line 7) (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unterminated string literal (detected at line 18) (<string>, line 18)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 9)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unterminated string literal (detected at line 6) (<string>, line 6)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n"
     ]
    }
   ],
   "source": [
    "# In recursive way, complete the KGs\n",
    "def process_arrays(arrays, index=0):\n",
    "    if index >= len(arrays):\n",
    "        return arrays\n",
    "\n",
    "    if not arrays[index]:\n",
    "        arrays[index] = batch_graph_eval.construct_kg_batch(train_generated_list[index:index+1])[0]\n",
    "\n",
    "    return process_arrays(arrays, index + 1)\n",
    "\n",
    "\n",
    "# Process the list of arrays\n",
    "processed_train_llm_kgs = process_arrays(train_llm_kgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9815f368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: unterminated string literal (detected at line 13) (<string>, line 13)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unterminated string literal (detected at line 6) (<string>, line 6)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 8)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unexpected indent (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 5)\n"
     ]
    }
   ],
   "source": [
    "# In recursive way, complete the KGs\n",
    "# Process the list of arrays\n",
    "re_processed_train_llm_kgs = process_arrays(processed_train_llm_kgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aca83c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 8)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: unterminated string literal (detected at line 6) (<string>, line 6)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n",
      "Error parsing LLM output: unterminated string literal (detected at line 7) (<string>, line 7)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 9)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 4)\n",
      "Error parsing LLM output: invalid syntax (<string>, line 7)\n"
     ]
    }
   ],
   "source": [
    "# In recursive way, complete the KGs\n",
    "# Process the list of arrays\n",
    "final_processed_train_llm_kgs = process_arrays(re_processed_train_llm_kgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training results ==> needs to be further fine-tuned\n",
    "batch_graph_eval = BatchGraphEval(llm_model=\"llama-3.3-70b-versatile\") #Specify Groq Model\n",
    "train_results = batch_graph_eval.evaluate_batch(final_processed_train_llm_kgs, train_grounding_list[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c5cae178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "# Example Articles and Summaries (replace with your actual data)\n",
    "articles = [\n",
    "    \"The Walt Disney Company, commonly known as Disney, is an American multinational mass media and entertainment conglomerate.\",\n",
    "    \"Amanda Jackson was born in Springfield, Ohio, USA on June 1, 1985. She was a basketball player for the U.S. women’s team.\",\n",
    "    \"Music executive Darius Van Arman was born in Pennsylvania. He attended Gonzaga College High School and is a human being.\",\n",
    "    \"Italy had 3.6x times more cases of coronavirus than China.\"\n",
    "]\n",
    "summaries = [\n",
    "    \"Disney is a media conglomerate.\",\n",
    "    \"Amanda Jackson was born in Ohio and played basketball.\",\n",
    "    \"Darius Van Arman is a music executive born in Pennsylvania\",\n",
    "    \"China had less coronavirus than Italy\"\n",
    "]\n",
    "\n",
    "# Example Labels (1 for consistent, 0 for inconsistent)\n",
    "labels = [1, 1, 1, 1]\n",
    "\n",
    "\n",
    "results = batch_graph_eval.evaluate_batch(articles, summaries)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9b2a51ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2878503/1015634075.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_train_data['prediction'] = predictions\n"
     ]
    }
   ],
   "source": [
    "# Store the results as a new column in the original DataFrame\n",
    "new_train_data = train_data.iloc[0:6112]\n",
    "new_train_data['prediction'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b6d9346a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2878503/3429334323.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_train_data['prediction'] = new_train_data['prediction'].replace(-1, 0)\n"
     ]
    }
   ],
   "source": [
    "new_train_data.loc[:, 'prediction'] = new_train_data['prediction'].replace(-1, 0)\n",
    "labels = list(new_train_data['label'])\n",
    "predicted_labels = list(new_train_data['prediction'])\n",
    "\n",
    "predicted_labels = list(new_train_data['prediction'])\n",
    "correct_predictions = sum([1 for i in range(len(labels)) if labels[i] == predicted_labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "73880a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>grounding</th>\n",
       "      <th>generated_text</th>\n",
       "      <th>label</th>\n",
       "      <th>cut</th>\n",
       "      <th>dataset_origin</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91198</td>\n",
       "      <td>Colin Kaepernick . Kaepernick began his profes...</td>\n",
       "      <td>Colin Kaepernick became a starting quarterback...</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>Fever</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194462</td>\n",
       "      <td>Katherine Matilda `` Tilda '' Swinton ( born 5...</td>\n",
       "      <td>Tilda Swinton is a vegan.</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>Fever</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137334</td>\n",
       "      <td>Soul Food is a 1997 American comedy-drama film...</td>\n",
       "      <td>Fox 2000 Pictures released the film Soul Food.</td>\n",
       "      <td>1</td>\n",
       "      <td>val</td>\n",
       "      <td>Fever</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111897</td>\n",
       "      <td>Telemundo ( [ teleˈmundo ] ) is an American Sp...</td>\n",
       "      <td>Telemundo is a English-language television net...</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>Fever</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>181634</td>\n",
       "      <td>Mogadishu ( [ ˌmɔːɡəˈdiːʃuː ] Muqdisho [ mʉqdɪ...</td>\n",
       "      <td>There is a capital called Mogadishu.</td>\n",
       "      <td>1</td>\n",
       "      <td>val</td>\n",
       "      <td>Fever</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>219028</td>\n",
       "      <td>Savages (2012 film) . Savages is a 2012 Americ...</td>\n",
       "      <td>Savages was exclusively a German film.</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>Fever</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>108281</td>\n",
       "      <td>Andrew Kevin Walker ( born August 14 , 1964 ) ...</td>\n",
       "      <td>Andrew Kevin Walker is only Chinese.</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>Fever</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>140846</td>\n",
       "      <td>Shooter (2007 film) . The film follows Force R...</td>\n",
       "      <td>Shooter is about an expert marksman who tries ...</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>Fever</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>54168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Murda Beatz's real name is Marshall Mathers.</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>Fever</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>105095</td>\n",
       "      <td>Carrie Anne Mathison , played by actress Clair...</td>\n",
       "      <td>Nicholas Brody is a character on Homeland.</td>\n",
       "      <td>1</td>\n",
       "      <td>val</td>\n",
       "      <td>Fever</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                          grounding  \\\n",
       "0    91198  Colin Kaepernick . Kaepernick began his profes...   \n",
       "1   194462  Katherine Matilda `` Tilda '' Swinton ( born 5...   \n",
       "2   137334  Soul Food is a 1997 American comedy-drama film...   \n",
       "4   111897  Telemundo ( [ teleˈmundo ] ) is an American Sp...   \n",
       "6   181634  Mogadishu ( [ ˌmɔːɡəˈdiːʃuː ] Muqdisho [ mʉqdɪ...   \n",
       "7   219028  Savages (2012 film) . Savages is a 2012 Americ...   \n",
       "9   108281  Andrew Kevin Walker ( born August 14 , 1964 ) ...   \n",
       "10  140846  Shooter (2007 film) . The film follows Force R...   \n",
       "13   54168                                                NaN   \n",
       "14  105095  Carrie Anne Mathison , played by actress Clair...   \n",
       "\n",
       "                                       generated_text  label  cut  \\\n",
       "0   Colin Kaepernick became a starting quarterback...      0  val   \n",
       "1                           Tilda Swinton is a vegan.      0  val   \n",
       "2      Fox 2000 Pictures released the film Soul Food.      1  val   \n",
       "4   Telemundo is a English-language television net...      0  val   \n",
       "6                There is a capital called Mogadishu.      1  val   \n",
       "7              Savages was exclusively a German film.      0  val   \n",
       "9                Andrew Kevin Walker is only Chinese.      0  val   \n",
       "10  Shooter is about an expert marksman who tries ...      0  val   \n",
       "13       Murda Beatz's real name is Marshall Mathers.      0  val   \n",
       "14         Nicholas Brody is a character on Homeland.      1  val   \n",
       "\n",
       "   dataset_origin  prediction  \n",
       "0           Fever           0  \n",
       "1           Fever           0  \n",
       "2           Fever           1  \n",
       "4           Fever           0  \n",
       "6           Fever           0  \n",
       "7           Fever           1  \n",
       "9           Fever           0  \n",
       "10          Fever           1  \n",
       "13          Fever           0  \n",
       "14          Fever           0  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "eeb76a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6277814136125655"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "correct_predictions/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b7280a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = sum([1 for i in range(len(labels)) if labels[i] == predicted_labels[i] and predicted_labels[i] == 1]) \n",
    "fp = sum([1 for i in range(len(labels)) if labels[i] != predicted_labels[i] and predicted_labels[i] == 1]) \n",
    "tn = sum([1 for i in range(len(labels)) if labels[i] == predicted_labels[i] and predicted_labels[i] == 0]) \n",
    "fn = sum([1 for i in range(len(labels)) if labels[i] != predicted_labels[i] and predicted_labels[i] == 0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "77683904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40016433853738703\n"
     ]
    }
   ],
   "source": [
    "precision = tp/(tp+fp)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "500d7347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23966535433070865\n"
     ]
    }
   ],
   "source": [
    "recall = tp/(tp+fn)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f14b8fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = train_data.iloc[0:6112]\n",
    "labels = list(new_train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c647505",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_label = sum([1 for i in range(len(labels)) if labels[i] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e16c687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3324607329842932"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_label/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c8fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dsgt_cleff_simpltxt_2]",
   "language": "python",
   "name": "conda-env-.conda-dsgt_cleff_simpltxt_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
