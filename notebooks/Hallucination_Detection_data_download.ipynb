{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fdb2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, pandas as pd, numpy as np, csv\n",
    "import requests\n",
    "import io\n",
    "import tarfile\n",
    "import zipfile\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e939254",
   "metadata": {},
   "source": [
    "### download datasets in to data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ba5605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XSUM data\n",
    "def download_XSUM():\n",
    "    # Convert the JSON data to a pandas DataFrame\n",
    "    df = pd.read_json('https://raw.githubusercontent.com/tanyuqian/ctc-gen-eval/master/train/data/qags_xsum.json')\n",
    "    \n",
    "    # Create the new column 'Id' by concatenating the row index with 'qags_xsum'\n",
    "    df['id'] = 'qags_xsum' + df.index.astype(str)\n",
    "    \n",
    "    # Randomly assign 70% of the rows to 'val' and 30% to 'test'\n",
    "    df['cut'] = np.where(np.random.rand(len(df)) < 0.7, 'val', 'test')\n",
    "    \n",
    "    df['dataset_origin'] = 'qags_xsum'\n",
    "    \n",
    "    # Define new column names\n",
    "    new_column_names = {'document': 'grounding', 'summary': 'generated_text', 'consistency': 'label'}\n",
    "    \n",
    "    # Rename the columns\n",
    "    df.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "    # Define the list of columns to subset\n",
    "    columns_to_subset = ['id', 'grounding', 'generated_text', 'label', 'cut', 'dataset_origin']\n",
    "    \n",
    "    # Create the subset DataFrame\n",
    "    xsum_df = df[columns_to_subset]\n",
    "\n",
    "    # Directory where the CSV file will be saved\n",
    "    save_dir = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save the subset DataFrame as a CSV file\n",
    "    xsum_df.to_csv(os.path.join(save_dir, 'qags_xsum_download.csv'), index=False)\n",
    "\n",
    "download_XSUM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9336f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN/DM data\n",
    "def download_CNNDM():\n",
    "    \n",
    "    # Convert the JSON data to a pandas DataFrame\n",
    "    df = pd.read_json('https://raw.githubusercontent.com/tanyuqian/ctc-gen-eval/master/train/data/qags_cnndm.json')\n",
    "    \n",
    "    # Create the new column 'Id' by concatenating the row index with 'qags_xsum'\n",
    "    df['id'] = 'qags_cnndm' + df.index.astype(str)\n",
    "    \n",
    "    # Randomly assign 70% of the rows to 'val' and 30% to 'test'\n",
    "    df['cut'] = np.where(np.random.rand(len(df)) < 0.7, 'val', 'test')\n",
    "    \n",
    "    df['dataset_origin'] = 'qags_cnndm'\n",
    "    \n",
    "    # Create a new column based on the value of 'consistency'\n",
    "    df['label'] = df['consistency'].apply(lambda x: 1 if x == 1 else 0)\n",
    "    \n",
    "    # Define new column names\n",
    "    new_column_names = {'document': 'grounding', 'summary': 'generated_text'}\n",
    "    \n",
    "    # Rename the columns\n",
    "    df.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "    # Define the list of columns to subset\n",
    "    columns_to_subset = ['id', 'grounding', 'generated_text', 'label', 'cut', 'dataset_origin']\n",
    "    \n",
    "    # Create the subset DataFrame\n",
    "    cnndm_df = df[columns_to_subset]\n",
    "\n",
    "    # Directory where the CSV file will be saved\n",
    "    save_dir = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save the subset DataFrame as a CSV file\n",
    "    cnndm_df.to_csv(os.path.join(save_dir, 'qags_cnndm_download.csv'), index=False)\n",
    "\n",
    "download_CNNDM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efab27d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggre-Fact data\n",
    "def download_AggreFact():\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv('https://raw.githubusercontent.com/Liyan06/AggreFact/refs/heads/main/data/aggre_fact_final.csv')\n",
    "\n",
    "    # Define new column names\n",
    "    new_column_names = {'doc': 'grounding', 'summary': 'generated_text', 'dataset': 'dataset_origin'}\n",
    "    \n",
    "    # Rename the columns\n",
    "    df.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "    # Define the list of columns to subset\n",
    "    columns_to_subset = ['id', 'grounding', 'generated_text', 'label', 'cut', 'dataset_origin']\n",
    "    \n",
    "    # Create the subset DataFrame\n",
    "    aggrefact_df = df[columns_to_subset]\n",
    "\n",
    "    # Directory where the CSV file will be saved\n",
    "    save_dir = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save the subset DataFrame as a CSV file\n",
    "    aggrefact_df.to_csv(os.path.join(save_dir, 'aggre_fact_download.csv'), index=False)\n",
    "        \n",
    "download_AggreFact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81fd7267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True data - download PAWS, FEVER and VitaminC datasets\n",
    "\n",
    "# PAWS\n",
    "def download_paws():\n",
    "    response = requests.get('https://storage.googleapis.com/paws/english/paws_wiki_labeled_final.tar.gz',stream=True)\n",
    "    with tarfile.open(fileobj=io.BytesIO(response.raw.read()), mode='r:gz') as tar_file:\n",
    "        f = tar_file.extractfile('final/dev.tsv')\n",
    "        df = pd.read_csv(f, sep='\\t')\n",
    "        df = df.rename(columns={'sentence1': 'grounding','sentence2': 'generated_text'})\n",
    "        # Randomly assign 70% of the rows to 'val' and 30% to 'test'\n",
    "        df['cut'] = np.where(np.random.rand(len(df)) < 0.7, 'val', 'test')\n",
    "        \n",
    "        df['dataset_origin'] = 'PAWS'\n",
    "        \n",
    "        \n",
    "        # Directory where the CSV file will be saved\n",
    "        save_dir = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # Path to save the CSV file\n",
    "        save_path = os.path.join(save_dir, 'paws_download.csv')\n",
    "        df.to_csv(save_path, index=False)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Vitamin C dataset\n",
    "def download_vitc():\n",
    "    response = requests.get(\n",
    "      'https://github.com/TalSchuster/talschuster.github.io/raw/master/static/vitaminc.zip',\n",
    "      stream=True)\n",
    "    with zipfile.ZipFile(io.BytesIO(response.raw.read())) as z:\n",
    "        \n",
    "        # Initialize an empty list to store DataFrames\n",
    "        dataframes = []\n",
    "        \n",
    "        # Extract and load each JSON file into a DataFrame\n",
    "        with z.open('vitaminc/dev.jsonl') as file:\n",
    "            df = pd.read_json(file, lines=True)\n",
    "            \n",
    "            # Randomly assign 70% of the rows to 'val' and 30% to 'test'\n",
    "            df['cut'] = np.where(np.random.rand(len(df)) < 0.7, 'val', 'test')\n",
    "            \n",
    "            df['label'] = df['label'].apply(lambda x: 1 if x == 'SUPPORTS' else 0)\n",
    "            dataframes.append(df)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        # Concatenate all DataFrames into one\n",
    "        df = pd.concat(dataframes, ignore_index=True)\n",
    "        df['dataset_origin'] = 'Vitamin C'\n",
    "        \n",
    "        # Reset the index of the concatenated DataFrame\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        new_column_names = {'unique_id':'id','evidence':'grounding', 'claim':'generated_text'}\n",
    "        # Rename the columns\n",
    "        df.rename(columns=new_column_names, inplace=True)\n",
    "        \n",
    "        # Define the list of columns to subset\n",
    "        columns_to_subset = ['id', 'grounding', 'generated_text', 'label', 'cut', 'dataset_origin']\n",
    "        \n",
    "        #Create the subset DataFrame\n",
    "        subset_df = df[columns_to_subset]\n",
    "        \n",
    "        # Directory where the CSV file will be saved\n",
    "        save_dir = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Path to save the CSV file\n",
    "        save_path = os.path.join(save_dir, 'vitc_download.csv')\n",
    "        \n",
    "        subset_df.to_csv(save_path, index=False)\n",
    "        \n",
    "       \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "download_paws()\n",
    "download_vitc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c294a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully and saved to /storage/coda1/p-dsgt_clef2025/0/kmarturi3/simpletext-2025-controlcreativity/data/nli_fever.zip\n"
     ]
    }
   ],
   "source": [
    "#Download Fever NLI zip file\n",
    "\n",
    "# URL of the Dropbox zip file\n",
    "url = 'https://www.dropbox.com/scl/fi/nyvxxz7n0hwwuvozmiknb/nli_fever.zip?rlkey=2yfvi7c2cxglyklexhgezioqr&e=1&dl=1'\n",
    "\n",
    "# Directory where the CSV file will be saved\n",
    "save_dir = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "        \n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "# Path to save the downloaded zip file\n",
    "save_path = os.path.join(save_dir, 'nli_fever.zip')\n",
    "\n",
    "# Download the zip file\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(save_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)\n",
    "    print(f'File downloaded successfully and saved to {save_path}')\n",
    "else:\n",
    "    print('Failed to download file')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe69a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of FEVER ids to labels using the FEVER dataset from HuggingFace Datasets\n",
    "def get_fever_labels(data_split = 'labelled_dev'):\n",
    "    id_to_label = {}\n",
    "    fever_data = load_dataset('fever', 'v1.0', split=data_split)\n",
    "    for example in fever_data:\n",
    "        id_to_label[example['id']] = example['label']\n",
    "    return id_to_label\n",
    "\n",
    "def download_fever():\n",
    "    save_dir = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "    zip_file_path = os.path.join(save_dir, 'nli_fever.zip')\n",
    "    \n",
    "    \n",
    "    # Create a directory to extract files if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    # Open the zip file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        \n",
    "        zip_ref.extract('nli_fever/dev_fitems.jsonl', save_dir)\n",
    "        \n",
    "        \n",
    "    # Extract and load each JSON file into a DataFrame\n",
    "    fever_id_to_label = get_fever_labels('labelled_dev')\n",
    "    df = pd.read_json(os.path.join(save_dir, 'nli_fever/dev_fitems.jsonl'), lines=True)\n",
    "    \n",
    "    # Randomly assign 70% of the rows to 'val' and 30% to 'test'\n",
    "    df['cut'] = np.where(np.random.rand(len(df)) < 0.7, 'val', 'test')\n",
    "    \n",
    "    df['label'] = df['cid'].apply(lambda x: 1 if fever_id_to_label[int(x)]  == 'SUPPORTS' else 0)\n",
    "    df['dataset_origin'] = 'Fever'\n",
    "    \n",
    "    \n",
    "        \n",
    "    new_column_names = {'cid':'id','context':'grounding', 'query':'generated_text'}\n",
    "    # Rename the columns\n",
    "    df.rename(columns=new_column_names, inplace=True)\n",
    "        \n",
    "    # Define the list of columns to subset\n",
    "    columns_to_subset = ['id', 'grounding', 'generated_text', 'label', 'cut', 'dataset_origin']\n",
    "        \n",
    "    #Create the subset DataFrame\n",
    "    subset_df = df[columns_to_subset]\n",
    "        \n",
    "    # Directory where the CSV file will be saved\n",
    "    #save_dir = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "        \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "    # Path to save the CSV file\n",
    "    save_path = os.path.join(save_dir, 'fever_download.csv')\n",
    "        \n",
    "    subset_df.to_csv(save_path, index=False)\n",
    "\n",
    "        \n",
    "download_fever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "359770f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download HaluEval and TofuEval \n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"achandlr/FactualConsistencyScoresTextSummarization\")\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "new_column_names = {'unique_id':'id','context':'grounding', 'summary':'generated_text', 'benchmark_origin':'dataset_origin'}\n",
    "# Rename the columns\n",
    "df.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "\n",
    "# Define the list of columns to subset\n",
    "columns_to_subset = ['id', 'grounding', 'generated_text', 'label', 'cut', 'dataset_origin']\n",
    "        \n",
    "#Create the subset DataFrame\n",
    "subset_df = df[columns_to_subset]\n",
    "\n",
    "#HaluEval dataset\n",
    "df_halu_eval = subset_df[subset_df['dataset_origin'] == 'HaluEval']\n",
    "df_halu_eval.reset_index(drop=True, inplace=True)\n",
    "# Randomly assign 70% of the rows to 'val' and 30% to 'test'\n",
    "halu_mask = np.random.rand(len(df_halu_eval)) < 0.7\n",
    "df_halu_eval.loc[halu_mask, 'cut'] = 'val'\n",
    "\n",
    "#TofuEval dataset\n",
    "df_tofu_eval = subset_df[subset_df['dataset_origin'] == 'TofuEval']\n",
    "df_tofu_eval.reset_index(drop=True, inplace=True)\n",
    "# Randomly assign 70% of the rows to 'val' and 30% to 'test'\n",
    "tofu_mask = np.random.rand(len(df_tofu_eval)) < 0.7\n",
    "df_tofu_eval.loc[tofu_mask, 'cut'] = 'val'\n",
    "\n",
    "# Directory where the CSV file will be saved\n",
    "save_dir = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "        \n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# save datasets as CSV file\n",
    "df_halu_eval.to_csv(os.path.join(save_dir, 'halueval_download.csv'), index=False)\n",
    "df_tofu_eval.to_csv(os.path.join(save_dir, 'tofueval_download.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddfbc64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#samsum data\n",
    "def download_samsum():\n",
    "    \n",
    "    # URL of the raw JSONL file on GitHub\n",
    "    jsonl_url = 'https://raw.githubusercontent.com/skgabriel/GoFigure/main/human_eval/samsum.jsonl'\n",
    "    \n",
    "    # Load the JSONL file into a pandas DataFrame\n",
    "    df = pd.read_json(jsonl_url, lines=True)\n",
    "    \n",
    "    # Create the new column 'Id' by concatenating the row index with 'qags_xsum'\n",
    "    df['id'] = 'samsum' + df.index.astype(str)\n",
    "    \n",
    "    # Randomly assign 70% of the rows to 'val' and 30% to 'test'\n",
    "    df['cut'] = np.where(np.random.rand(len(df)) < 0.7, 'val', 'test')\n",
    "    \n",
    "    df['dataset_origin'] = 'samsum'\n",
    "    \n",
    "    # Create a new column based on the value of 'consistency'\n",
    "    df['label'] = df['label'].apply(lambda x: 1 if x == 'factual' else 0)\n",
    "    \n",
    "    # Define new column names\n",
    "    new_column_names = {'article': 'grounding', 'summary': 'generated_text'}\n",
    "    \n",
    "    # Rename the columns\n",
    "    df.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "    # Define the list of columns to subset\n",
    "    columns_to_subset = ['id', 'grounding', 'generated_text', 'label', 'cut', 'dataset_origin']\n",
    "    \n",
    "    # Create the subset DataFrame\n",
    "    samsum_df = df[columns_to_subset]\n",
    "\n",
    "    # Directory where the CSV file will be saved\n",
    "    save_dir = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save the subset DataFrame as a CSV file\n",
    "    samsum_df.to_csv(os.path.join(save_dir, 'samsum_download.csv'), index=False)\n",
    "\n",
    "download_samsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79972c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dsgt_cleff_simpltxt_2]",
   "language": "python",
   "name": "conda-env-.conda-dsgt_cleff_simpltxt_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
